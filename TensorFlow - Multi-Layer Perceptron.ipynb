{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: Building out a Multi Layer Perceptron model and trying to classify hand written digits using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description: \n",
    "\n",
    "### Using the famous MNIST data set of handwritten digits.\n",
    " The images are black and white images of size 28 x 28 pixels, or 784 pixels total. \n",
    " Features will be the pixel values for each pixel. Either the pixel is \"white\" (blank with a 0), or there is some pixel value.\n",
    "#### Trying to correctly predict what number is written down based solely on the image data in the form of an array. \n",
    "(Image Recognition) is a great use case for Deep Learning Methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Importing MINST data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "The data is stored in a vector format, although the original data was a 2-dimensional matirx with values representing how much pigment was at a certain location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0509804 ,\n",
       "         0.4666667 ,  0.99607849,  0.99607849,  0.99607849,  0.96078438,\n",
       "         0.54509807,  0.21568629,  0.01960784,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.37647063,  0.83529419,\n",
       "         0.99215692,  0.99215692,  0.99215692,  0.99215692,  0.99607849,\n",
       "         0.99215692,  0.99215692,  0.53333336,  0.18431373,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.3921569 ,  0.96862751,  1.        ,\n",
       "         0.99215692,  0.99215692,  0.99215692,  0.99215692,  1.        ,\n",
       "         0.99215692,  0.99215692,  0.99215692,  0.99215692,  0.72941178,\n",
       "         0.26274511,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.65098041,  0.99215692,  0.99607849,\n",
       "         0.99215692,  0.99215692,  0.69411767,  0.7960785 ,  0.96470594,\n",
       "         0.34509805,  0.50588238,  0.92941183,  0.99215692,  0.99607849,\n",
       "         0.36078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.20000002,  0.95294124,  0.74901962,\n",
       "         0.45882356,  0.09019608,  0.01568628,  0.04313726,  0.08235294,\n",
       "         0.        ,  0.        ,  0.81568635,  0.99215692,  0.99607849,\n",
       "         0.36078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.19215688,  0.91764712,  0.99607849,  0.99607849,\n",
       "         0.32941177,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.03529412,  0.77254909,  0.99215692,  0.99215692,  0.82352948,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.02745098,\n",
       "         0.61176473,  0.99215692,  0.99215692,  0.99215692,  0.1254902 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.09411766,\n",
       "         0.99215692,  0.99215692,  0.99215692,  0.61960787,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.09411766,\n",
       "         0.99215692,  0.99215692,  0.82745105,  0.09019608,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.04313726,  0.7960785 ,\n",
       "         0.99607849,  0.99607849,  0.38823533,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.45490199,  0.99607849,\n",
       "         0.99215692,  0.73725492,  0.06666667,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.7843138 ,  0.99607849,\n",
       "         0.99215692,  0.27058825,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.13333334,  0.93725497,  0.99607849,\n",
       "         0.84313732,  0.0509804 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.1254902 ,  0.8588236 ,  0.99215692,  0.95686281,\n",
       "         0.23529413,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.10196079,  0.66666669,  0.99607849,  0.99607849,  0.8705883 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.26666668,  0.99215692,  0.99215692,  0.99215692,  0.37254903,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.63529414,  0.99215692,  0.99215692,  0.66274512,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.63529414,  0.99215692,  0.99215692,  0.53725493,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.43137258,  0.99215692,  0.82745105,  0.09019608,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = mnist.train.images[2017].reshape(28,28) # Can set random images \n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To visualize the above array \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xc8a7a6c9b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq3MOIDIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc8a79f5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matplot lib function(imshow) to show 2d matrix based on color scale.\n",
    "plt.imshow(sample,cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Since MNIST is pretty famous,some reasonable values for data are below. \n",
    "The parameters:\n",
    "\n",
    "Learning Rate - How quickly to adjust the cost function.\n",
    "Training Epochs - How many training cycles to go through.\n",
    "Batch Size - Size of the 'batches' of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Parameters\n",
    "\n",
    "The parameters which will directly define Neural Network, these would be adjusted depending on what data looks like and what kind of a net to build. Basically using some numbers eventually, and define some variables later on in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 256  # 1st layer number of features\n",
    "n_hidden_2 = 256  # 2nd layer number of features\n",
    "# More the layers more time to run the model. \n",
    "\n",
    "n_input = 784     # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10    # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating function\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    '''\n",
    "    x : Place Holder for Data Input\n",
    "    weights: Dictionary of weights\n",
    "    biases: Dicitionary of biases\n",
    "    '''\n",
    "    \n",
    " # First Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) # X * W + B\n",
    "    layer_1 = tf.nn.relu(layer_1) # RELU(X * W + B) -> f(x) = max(0,x)\n",
    "    \n",
    "# Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "# Last Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias\n",
    "Creating two dictionaries containing our weight and bias objects for the model. \n",
    "Using tf's built-in random_normal method to create the random values for weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Graph Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MultiLayer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constructing model\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss and optimizer #Using TensorFlow built-in cost & optimizer\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Variables\n",
    "\n",
    "Initializing all tf.Variable objects created earlier. This will be the first thing to run when training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=178.5591\n",
      "Epoch: 2 cost=41.1259\n",
      "Epoch: 3 cost=26.0991\n",
      "Epoch: 4 cost=18.3055\n",
      "Epoch: 5 cost=13.3953\n",
      "Epoch: 6 cost=9.8572\n",
      "Epoch: 7 cost=7.3609\n",
      "Epoch: 8 cost=5.4481\n",
      "Epoch: 9 cost=4.1653\n",
      "Epoch: 10 cost=3.0505\n",
      "Epoch: 11 cost=2.3722\n",
      "Epoch: 12 cost=1.6750\n",
      "Epoch: 13 cost=1.4569\n",
      "Epoch: 14 cost=0.9817\n",
      "Epoch: 15 cost=0.8333\n",
      "Model has completed 15 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "# Launching the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intializing all the variables\n",
    "sess.run(init)\n",
    "\n",
    "# Training Epochs\n",
    "# Essentially the max amount of loops possible before we stop\n",
    "# May stop earlier if cost/loss limit was set\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Starting with cost = 0.0\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    # Converting total number of batches to integer\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        # Getting next batch of training data and labels\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Feed dictionary for optimization and loss value\n",
    "        # Returns a tuple, but only need 'c' the cost\n",
    "        # So set an underscore as a \"throwaway\"\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Computing average loss\n",
    "        avg_cost += c / total_batch\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                 Understanding NN: \n",
    "Training the Neural network with same 55,000 data samples 15 times.  \n",
    "\n",
    "For One Epoch :\n",
    "\n",
    "The total data set is divided into 100 sets of 550 samples.\n",
    "\n",
    "So after the first 550 samples we calculate cost and try to reduce the cost by using the optimizer. And basically, Weight is updated after each 550 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_4:0\", shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using tf.cast to cast the Tensor of booleans back into a Tensor of Floating point values in order to take the mean of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_5:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean of the elements across the tensor.\n",
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9419\n"
     ]
    }
   ],
   "source": [
    "#Using eval() method to directly evaluate tensor in a `Session` without needing to call tf.sess():mm\n",
    "\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "94% isn't anywhere near as good as it could be.\n",
    "Running for more training epochs with data (around 20,000) can produce accuracy around 99%. \n",
    "But, I am not doing it here because that will take a very long time to run!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
